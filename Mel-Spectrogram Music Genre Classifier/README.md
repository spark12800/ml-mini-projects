# Mel-Spectrogram Music Genre Classifier

**Goal:**  

Classify music genres using a hybrid CNNâ€“LSTM network optimised for Mel-spectrogram inputs.

---

## Model Architecture

The model extracts timeâ€“frequency patterns using **two parallel CNN branches**, each capturing different receptive fields:

- **Branch 1:** Conv2D(32, 5Ã—5) â†’ LeakyReLU(0.3) â†’ MaxPool(4Ã—4) Ã— 2  
- **Branch 2:** Conv2D(32, 3Ã—3) â†’ LeakyReLU(0.3) â†’ MaxPool(2Ã—2) Ã— 2  

Features from both branches pass through **LSTM(128 â†’ 32)** layers, then are concatenated and fed to a Dense â†’ Softmax classifier.

Conv2D is used instead of Conv1D to preserve full timeâ€“frequency structure.

---

## Optimisation Strategy (Key Contribution)

This project focuses heavily on **systematic optimisation** of the modelâ€™s regularisation, training dynamics, and sequence modeling quality.

### ðŸ”¹ Regularisation & Stabilisation
- **Batch Normalisation** after every Conv2D to stabilise feature scaling  
- **Dropout (p = 0.2)** after pooling to reduce overfitting  
- **L2 weight regularisation**  
- **He normal initialisation** for variance-stable convolutional layers  
- **Leaky ReLU (Î± = 0.3)** to avoid dead activations and mitigate vanishing gradients  

### ðŸ”¹ Learning Rate 
A **manually designed multi-stage learning rate schedule** was implemented to balance fast initial convergence with stable late-stage optimisation:

- **Epochs 1â€“5:** linear warm-up from 5eâˆ’5  
- **Epochs 5â€“41:** main training at 5.55eâˆ’4  
- **Epochs 42â€“46:** reduced to 2eâˆ’4 for stabilisation  
- **Epochs 47â€“50:** final annealing to 5eâˆ’5  

<img src="image_mel_spectrogram/learning_rate.png" width="70%">


This schedule significantly improved convergence and prevented overshooting during early optimisation.

### ðŸ”¹ Data Augmentation 
Augmentations (noise, time-shift) were initially applied to enforce i.i.d. batches, but **empirically decreased validation accuracy** due to correlated song segments.  
Final model therefore trains **without augmentation**, confirming that optimisation choices should follow data dynamics, not assumptions.

---

## Key Findings

- Multi-kernel CNN branches improve generalisation by capturing features at different scales.  
- LSTMs effectively model temporal dependencies not captured by CNNs alone.  
- The custom LR schedule and BNâ€“Dropout combination are crucial for overcoming early overfitting.  
- Empirical evaluation showed augmentation was counterproductive given dataset structure.  

---
## Result
<img src="image_mel_spectrogram/performance_graph.png" width="90%">

Achieved 93% in validation set.

---

**Tools:** Tensorflow, KerasTuner 

**Notebook:** `mel_spectrogram_classifier.ipynb`

